{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Climate Change - Data Preparation\n",
    "Supervision: Prof. Dr. Jan Fabian Ehmke\n",
    "\n",
    "Group members: Britz Luis, Huber Anja, Krause Felix Elias, Preda Yvonne-Nadine\n",
    "\n",
    "Time: Summer term 2023 \n",
    "\n",
    "Data: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in /opt/homebrew/lib/python3.10/site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from langid) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Loading packages\n",
    "%pip install langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from utils.nltk_function_samples import check_lang\n",
    "from utils.nltk_function_samples import remove_features\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "raw_comments = pd.read_csv('data/the-reddit-climate-change-dataset-comments.csv')\n",
    "raw_posts = pd.read_csv('data/the-reddit-climate-change-dataset-posts.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in post and comment dataset\n",
    "clean_posts = raw_posts.drop_duplicates(subset=[\"selftext\",\"type\"])\n",
    "clean_comments = raw_comments.drop_duplicates(subset=[\"body\",\"type\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted feature removal function\n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    \n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "    \n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "\n",
    "    cleaned_str = data_str\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_97033/4067014042.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_posts[\"selftext\"] = clean_posts[\"selftext\"].astype(str)\n",
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_97033/4067014042.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_posts[\"title\"] = clean_posts[\"title\"].astype(str)\n",
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_97033/4067014042.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_comments[\"body\"] = clean_comments[\"body\"].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Convert text columns to be of type string\n",
    "clean_posts[\"selftext\"] = clean_posts[\"selftext\"].astype(str)\n",
    "clean_posts[\"title\"] = clean_posts[\"title\"].astype(str)\n",
    "clean_comments[\"body\"] = clean_comments[\"body\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_97033/678669613.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_comments[\"body_clean\"] = clean_comments[\"body\"].apply(remove_features)\n",
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_97033/678669613.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_posts[\"title_clean\"] = clean_posts[\"title\"].apply(remove_features)\n",
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_97033/678669613.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_posts[\"selftext_clean\"] = clean_posts[\"selftext\"].apply(remove_features)\n"
     ]
    }
   ],
   "source": [
    "# Apply remove feature function to all text data and create new columns with clean text\n",
    "clean_comments[\"body_clean\"] = clean_comments[\"body\"].apply(remove_features)\n",
    "clean_posts[\"title_clean\"] = clean_posts[\"title\"].apply(remove_features)\n",
    "clean_posts[\"selftext_clean\"] = clean_posts[\"selftext\"].apply(remove_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort out specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort out the word climate change in comment and post dataset\n",
    "deleted_words = (\"climate\",\"change\")\n",
    "\n",
    "for x in deleted_words:\n",
    "    clean_posts[\"selftext_clean\"] = clean_posts[\"selftext_clean\"].str.replace(x, \"\", case=False)\n",
    "    clean_posts[\"title_clean\"] = clean_posts[\"title_clean\"].str.replace(x, \"\", case=False)\n",
    "    clean_comments[\"body_clean\"] = clean_comments[\"body_clean\"].str.replace(x, \"\", case=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty, removed and deleted entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean post data set from empty posts, removed posts, deleted posts\n",
    "clean_posts = clean_posts.drop(clean_posts[clean_posts['selftext_clean'] == '[removed]'].index)\n",
    "clean_posts = clean_posts.drop(clean_posts[clean_posts['selftext_clean'] == '[deleted]'].index)\n",
    "clean_posts = clean_posts.dropna(subset=['selftext_clean'])\n",
    "clean_posts = clean_posts.drop(clean_posts[clean_posts['title_clean'] == ' '].index) # empty title because climate change was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean comment dataset from empty comments\n",
    "clean_comments = clean_comments.dropna(subset=['body_clean'], how='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_subreddits = ['memebot9000', \n",
    "                  'spacenewsbot', \n",
    "                  'removalbot', \n",
    "                  'u_ignorantbotkin4ho', \n",
    "                  'botsrights', \n",
    "                  'gildedbot', \n",
    "                  'open_bots_test', \n",
    "                  'rcbredditbot', \n",
    "                  'newsbotmarket', \n",
    "                  'alt_source_bot_log', \n",
    "                  'steamiebot', \n",
    "                  'modbot_staging',\n",
    "                  'testanimalsupportbot', \n",
    "                  'u_udemy_sample_bot', \n",
    "                  'newsbottmt', \n",
    "                  'bottownfriends', \n",
    "                  'airsoft_bot', \n",
    "                  'wikileaksemailbot', \n",
    "                  'foreveralonebots', \n",
    "                  'botterminator', \n",
    "                  'bottown2', \n",
    "                  'bottown_polibot', \n",
    "                  'bottowngarden', \n",
    "                  'havoc_bot', \n",
    "                  'botrequests', \n",
    "                  'sentimentviewbot', \n",
    "                  'newsbotscience', \n",
    "                  'u_commonmisspellingbot', \n",
    "                  'dogetipbot', \n",
    "                  'resistbot', \n",
    "                  'newsbotfunding', \n",
    "                  'ebolanewsbot', \n",
    "                  'gwcoepbot', \n",
    "                  'uknewsbyabot', \n",
    "                  'u_flamboyantbotoshrx', \n",
    "                  'laserlikebot', \n",
    "                  'repostsleuthbot', \n",
    "                  'interfaithbotdialogue', \n",
    "                  'articlebot', \n",
    "                  'potuswatchbot', \n",
    "                  'pulsarbot', \n",
    "                  'mimeticsbot', \n",
    "                  'cryptobots', \n",
    "                  'wutbotposts', \n",
    "                  'bottesting', \n",
    "                  'botbotread', \n",
    "                  'cleverbot', \n",
    "                  'u_anticensor_bot', \n",
    "                  'trollbot', \n",
    "                  'brokentranslatebot', \n",
    "                  'newsbiasbot', \n",
    "                  'sexpollbottest', \n",
    "                  'bottalks', \n",
    "                  'bitnewsbot', \n",
    "                  'gbpolbot', \n",
    "                  'rssbot', \n",
    "                  'botsscrewingup', \n",
    "                  'u_dicebotbtc', \n",
    "                  'u_userleansbot', \n",
    "                  'islamicstatenewsbot', \n",
    "                  'u_moebot12', \n",
    "                  'webbot', \n",
    "                  'pew_bot_memes', \n",
    "                  'u_zukbot', \n",
    "                  'u_bot4bot', \n",
    "                  'stabbot', \n",
    "                  'u_yangpolicyinfo_bot', \n",
    "                  'subredditsummarybot', \n",
    "                  'bot4bottesting', \n",
    "                  'travsbots', \n",
    "                  'spooktoberbot', \n",
    "                  'trollabot', \n",
    "                  'printrbot', \n",
    "                  'bottown1', \n",
    "                  'kzreminderbotsub', \n",
    "                  'bottown', \n",
    "                  'newsbotbot', \n",
    "                  'gifbot', \n",
    "                  'uvabot', \n",
    "                  'cfbotpolitics', \n",
    "                  'nwordcountbot', \n",
    "                  'quizzybot', \n",
    "                  'israelnewsbot', \n",
    "                  'popularnewsbot', \n",
    "                  'monkeynewsbot', \n",
    "                  'thelinkfixerbot', \n",
    "                  'atheismbot', \n",
    "                  'inspirobotbot', \n",
    "                  'testingground4bots', \n",
    "                  'buzzfeedbot', \n",
    "                  'isreactionarybot', \n",
    "                  'blenderbot', \n",
    "                  'talkwithgpt2bots', \n",
    "                  'bottown22', \n",
    "                  'twitter_bot', \n",
    "                  'autowikibot', \n",
    "                  'elsbot']\n",
    "\n",
    "for i in bot_subreddits:\n",
    "    clean_comments = clean_comments[~clean_comments['subreddit.name'].str.contains(i)]\n",
    "    clean_posts = clean_posts[~clean_posts['subreddit.name'].str.contains(i)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add language information\n",
    "clean_comments[\"language\"] = clean_comments[\"body_clean\"].apply(check_lang)\n",
    "clean_posts[\"language\"] = clean_posts[\"selftext_clean\"].apply(check_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting out all not english posts and comments, assumption that post body represents language\n",
    "clean_posts = clean_posts[clean_posts[\"language\"]==\"en\"]\n",
    "clean_comments = clean_comments[clean_comments[\"language\"]==\"en\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date and time information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new columns with date and time information\n",
    "clean_posts['created_date'] = pd.to_datetime(clean_posts['created_utc'], utc=True, unit='s').dt.strftime('%Y-%m-%d')\n",
    "clean_posts['created_day'] = pd.to_datetime(clean_posts['created_utc'], utc=True, unit='s').dt.strftime('%d')\n",
    "clean_posts['created_month'] = pd.to_datetime(clean_posts['created_utc'], utc=True, unit='s').dt.strftime('%m')\n",
    "clean_posts['created_year'] = pd.to_datetime(clean_posts['created_utc'], utc=True, unit='s').dt.strftime('%Y')\n",
    "clean_posts['created_time'] = pd.to_datetime(clean_posts['created_utc'], utc=True, unit='s').dt.strftime('%H:%M:%S')\n",
    "\n",
    "clean_comments['created_date'] = pd.to_datetime(clean_comments['created_utc'], utc=True, unit='s').dt.strftime('%Y-%m-%d')\n",
    "clean_comments['created_day'] = pd.to_datetime(clean_comments['created_utc'], utc=True, unit='s').dt.strftime('%d')\n",
    "clean_comments['created_month'] = pd.to_datetime(clean_comments['created_utc'], utc=True, unit='s').dt.strftime('%m')\n",
    "clean_comments['created_year'] = pd.to_datetime(clean_comments['created_utc'], utc=True, unit='s').dt.strftime('%Y')\n",
    "clean_comments['created_time'] = pd.to_datetime(clean_comments['created_utc'], utc=True, unit='s').dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \"posts\" CSV file in data folder\n",
    "posts_save_path = f\"data/posts_preprocessed.csv\"\n",
    "\n",
    "if not os.path.isfile(posts_save_path):\n",
    "    clean_posts.to_csv('data/preprocessed_posts.csv', index=False)\n",
    "    print(\"File saved!\")\n",
    "else:\n",
    "    print(\"Warning file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce comment file size by dropping unnecessary columns because it got too large\n",
    "clean_comments = clean_comments.drop([\"type\",\"subreddit.id\",\"body\",\"language\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to wrong commata in the original file there are NA rows created which should be deleted\n",
    "clean_comments = clean_comments.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \"comments\" CSV file in data folder\n",
    "comments_save_path = f\"data/comments_preprocessed.csv\"\n",
    "\n",
    "if not os.path.isfile(comments_save_path):\n",
    "    clean_comments.to_csv(\"data/preprocessed_comments\" + '.gzip', index=False, compression='gzip')\n",
    "    print(\"File saved!\")\n",
    "else:\n",
    "    print(\"Warning file already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
