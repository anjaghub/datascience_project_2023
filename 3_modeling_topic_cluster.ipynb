{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Climate Change - Modeling & Evaluation\n",
    "Supervision: Prof. Dr. Jan Fabian Ehmke\n",
    "\n",
    "Group members: Britz Luis, Huber Anja, Krause Felix Elias, Preda Yvonne-Nadine\n",
    "\n",
    "Time: Summer term 2023 \n",
    "\n",
    "Data: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Topic detection\n",
    "\n",
    "# LDA\n",
    "# https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "# http://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "\n",
    "# BERTopic\n",
    "# https://maartengr.github.io/BERTopic/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.14.1-py2.py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/homebrew/lib/python3.10/site-packages (from bertopic) (1.24.3)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Using cached hdbscan-0.8.29-cp310-cp310-macosx_10_9_universal2.whl\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Using cached umap_learn-0.5.3-py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/homebrew/lib/python3.10/site-packages (from bertopic) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/homebrew/lib/python3.10/site-packages (from bertopic) (1.2.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/homebrew/lib/python3.10/site-packages (from bertopic) (4.65.0)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting plotly>=4.7.0 (from bertopic)\n",
      "  Downloading plotly-5.14.1-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cython>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
      "  Using cached Cython-0.29.34-py2.py3-none-any.whl (988 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anja/Library/Python/3.10/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Collecting tenacity>=6.2.0 (from plotly>=4.7.0->bertopic)\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: packaging in /Users/anja/Library/Python/3.10/lib/python/site-packages (from plotly>=4.7.0->bertopic) (23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached torch-2.0.0-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "Collecting torchvision (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached torchvision-0.15.1-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Collecting numba>=0.49 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading numba-0.57.0-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Using cached pynndescent-0.5.10-py3-none-any.whl\n",
      "Collecting filelock (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "Collecting requests (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1 (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached PyYAML-6.0-cp310-cp310-macosx_11_0_arm64.whl (173 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.49->umap-learn>=0.5.0->bertopic)\n",
      "  Downloading llvmlite-0.40.0-cp310-cp310-macosx_11_0_arm64.whl (28.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.1/28.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/anja/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Collecting sympy (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting networkx (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting jinja2 (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2023.3.23)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.10/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.5.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading MarkupSafe-2.1.2-cp310-cp310-macosx_10_9_universal2.whl (17 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading charset_normalizer-3.1.0-cp310-cp310-macosx_11_0_arm64.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.0/123.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5 (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tokenizers, sentencepiece, mpmath, urllib3, typing-extensions, tenacity, sympy, pyyaml, networkx, MarkupSafe, llvmlite, idna, fsspec, filelock, cython, charset-normalizer, certifi, requests, plotly, numba, jinja2, torch, pynndescent, huggingface-hub, hdbscan, umap-learn, transformers, torchvision, sentence-transformers, bertopic\n",
      "Successfully installed MarkupSafe-2.1.2 bertopic-0.14.1 certifi-2022.12.7 charset-normalizer-3.1.0 cython-0.29.34 filelock-3.12.0 fsspec-2023.4.0 hdbscan-0.8.29 huggingface-hub-0.14.1 idna-3.4 jinja2-3.1.2 llvmlite-0.40.0 mpmath-1.3.0 networkx-3.1 numba-0.57.0 plotly-5.14.1 pynndescent-0.5.10 pyyaml-6.0 requests-2.29.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.11.1 tenacity-8.2.2 tokenizers-0.13.3 torch-2.0.0 torchvision-0.15.1 transformers-4.28.1 typing-extensions-4.5.0 umap-learn-0.5.3 urllib3-1.26.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# Preparing environment\n",
    "%pip install bertopic\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_1980/2118955341.py:3: DtypeWarning: Columns (0,1,2,3,4,5,6,7,10,11,12,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  clean_comments = pd.read_csv(\"data/preprocessed_comments.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "clean_posts = pd.read_csv(\"data/preprocessed_posts.csv\")\n",
    "clean_comments = pd.read_csv(\"data/preprocessed_comments.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Topic Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of posts because otherwise it's to much data\n",
    "subset_comments = clean_comments[(clean_comments['created_year'] == 2010)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one array of all titles, to feed it into BERT\n",
    "docs = subset_comments.body_clean.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 500/500 [04:10<00:00,  2.00it/s]\n",
      "2023-05-03 10:44:09,965 - BERTopic - Transformed documents to Embeddings\n",
      "2023-05-03 10:44:40,232 - BERTopic - Reduced dimensionality\n",
      "2023-05-03 10:44:40,674 - BERTopic - Clustered reduced embeddings\n",
      "2023-05-03 10:44:45,574 - BERTopic - Reduced number of topics from 106 to 74\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>8554</td>\n",
       "      <td>-1_people_gt_global_just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1544</td>\n",
       "      <td>0_science_people_gt_don</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1154</td>\n",
       "      <td>1_weather_warming_global_snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>505</td>\n",
       "      <td>2_temperature_atmosphere_water_vapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>315</td>\n",
       "      <td>3_obama_rudd_labor_party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>68_grassroots_poptart_article_typety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>69</td>\n",
       "      <td>16</td>\n",
       "      <td>69_causation_correlation_does_equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>70_economics_economists_nafta_maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>71</td>\n",
       "      <td>15</td>\n",
       "      <td>71_ica_water_peru_aquifer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "      <td>72_exxonmobil_exxon_million_groups</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                  Name\n",
       "0      -1   8554              -1_people_gt_global_just\n",
       "1       0   1544               0_science_people_gt_don\n",
       "2       1   1154         1_weather_warming_global_snow\n",
       "3       2    505  2_temperature_atmosphere_water_vapor\n",
       "4       3    315              3_obama_rudd_labor_party\n",
       "..    ...    ...                                   ...\n",
       "69     68     16  68_grassroots_poptart_article_typety\n",
       "70     69     16   69_causation_correlation_does_equal\n",
       "71     70     16   70_economics_economists_nafta_maths\n",
       "72     71     15             71_ica_water_peru_aquifer\n",
       "73     72     15    72_exxonmobil_exxon_million_groups\n",
       "\n",
       "[74 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT stepwise\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,    # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,              # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,        # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,  # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,          # Step 5 - Extract topic words\n",
    "  calculate_probabilities=False,      # Raises speed\n",
    "  min_topic_size = 300,               # Reduces number of topics\n",
    "  nr_topics=\"auto\",                    # Reduces number of topics\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 0.01219470217097759),\n",
       " ('people', 0.010584660869951001),\n",
       " ('gt', 0.009517132731414605),\n",
       " ('don', 0.0086000351186167),\n",
       " ('scientific', 0.008597060961067942),\n",
       " ('global', 0.00822324026848448),\n",
       " ('think', 0.00787297838775973),\n",
       " ('scientists', 0.007625308000465114),\n",
       " ('evolution', 0.007521849591286694),\n",
       " ('warming', 0.007371242329106124)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get specific topic\n",
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store topic info in dataframe\n",
    "doc_info = topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Industrial output --&amp;gt; Increased atmospheric...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_people_gt_global_just</td>\n",
       "      <td>people - gt - global - just - science - don - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is true but only because Australia lacks ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_people_gt_global_just</td>\n",
       "      <td>people - gt - global - just - science - don - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please, explain to us the whole concept.\\n\\nNe...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_people_gt_global_just</td>\n",
       "      <td>people - gt - global - just - science - don - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It hasn't been \"d\" some political types prefer...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_weather_warming_global_snow</td>\n",
       "      <td>weather - warming - global - snow - ice - temp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&amp;gt; It's called \" \" or as the right wing has ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_weather_warming_global_snow</td>\n",
       "      <td>weather - warming - global - snow - ice - temp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Topic   \n",
       "0  Industrial output --&gt; Increased atmospheric...     -1  \\\n",
       "1  This is true but only because Australia lacks ...     -1   \n",
       "2  Please, explain to us the whole concept.\\n\\nNe...     -1   \n",
       "3  It hasn't been \"d\" some political types prefer...      1   \n",
       "4  &gt; It's called \" \" or as the right wing has ...      1   \n",
       "\n",
       "                            Name   \n",
       "0       -1_people_gt_global_just  \\\n",
       "1       -1_people_gt_global_just   \n",
       "2       -1_people_gt_global_just   \n",
       "3  1_weather_warming_global_snow   \n",
       "4  1_weather_warming_global_snow   \n",
       "\n",
       "                                         Top_n_words  Probability   \n",
       "0  people - gt - global - just - science - don - ...          0.0  \\\n",
       "1  people - gt - global - just - science - don - ...          0.0   \n",
       "2  people - gt - global - just - science - don - ...          0.0   \n",
       "3  weather - warming - global - snow - ice - temp...          1.0   \n",
       "4  weather - warming - global - snow - ice - temp...          1.0   \n",
       "\n",
       "   Representative_document  \n",
       "0                    False  \n",
       "1                    False  \n",
       "2                    False  \n",
       "3                    False  \n",
       "4                    False  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out document information\n",
    "doc_info.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
