{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Climate Change - Modeling & Evaluation\n",
    "Supervision: Prof. Dr. Jan Fabian Ehmke\n",
    "\n",
    "Group members: Britz Luis, Huber Anja, Krause Felix Elias, Preda Yvonne-Nadine\n",
    "\n",
    "Time: Summer term 2023 \n",
    "\n",
    "Data: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Topic detection\n",
    "\n",
    "# LDA\n",
    "# https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "# http://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "\n",
    "# BERTopic\n",
    "# https://maartengr.github.io/BERTopic/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/opt/homebrew/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# Preparing environment\n",
    "#%pip install bertopic\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/kbz524cd32j80_2tm_hwnr780000gn/T/ipykernel_96121/3432851145.py:2: DtypeWarning: Columns (0,1,2,4,7,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  clean_comments = pd.read_csv(\"data/preprocessed_comments.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "clean_comments = pd.read_csv(\"data/preprocessed_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit.name</th>\n",
       "      <th>subreddit.nsfw</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_day</th>\n",
       "      <th>created_month</th>\n",
       "      <th>created_year</th>\n",
       "      <th>created_time</th>\n",
       "      <th>body_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188548</th>\n",
       "      <td>h6kpauz</td>\n",
       "      <td>polls</td>\n",
       "      <td>False</td>\n",
       "      <td>1.627298e+09</td>\n",
       "      <td>https://old.reddit.com/r/polls/comments/orl7mj...</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>11:17:44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188549</th>\n",
       "      <td>Stop climate change and start WW  because WW  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494626</th>\n",
       "      <td>- Targeted cuts to research on climate change</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494627</th>\n",
       "      <td>- Openly calling for killing civilians in the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518932</th>\n",
       "      <td>You *walk on* a recording of climate change  *...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376256</th>\n",
       "      <td>.html</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385567</th>\n",
       "      <td>Being skeptical of this is as bad as denying t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394370</th>\n",
       "      <td>German school children still write letters to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424093</th>\n",
       "      <td>Edit: a word</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441792</th>\n",
       "      <td>selling as one (of many) outcome of climate c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329562 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        id subreddit.name   \n",
       "188548                                             h6kpauz          polls  \\\n",
       "188549   Stop climate change and start WW  because WW  ...            NaN   \n",
       "494626       - Targeted cuts to research on climate change            NaN   \n",
       "494627   - Openly calling for killing civilians in the ...            NaN   \n",
       "518932   You *walk on* a recording of climate change  *...            NaN   \n",
       "...                                                    ...            ...   \n",
       "1376256                                              .html            NaN   \n",
       "1385567  Being skeptical of this is as bad as denying t...            NaN   \n",
       "1394370  German school children still write letters to ...            NaN   \n",
       "1424093                                       Edit: a word            NaN   \n",
       "1441792   selling as one (of many) outcome of climate c...            NaN   \n",
       "\n",
       "        subreddit.nsfw   created_utc   \n",
       "188548           False  1.627298e+09  \\\n",
       "188549             NaN           NaN   \n",
       "494626             NaN           NaN   \n",
       "494627             NaN           NaN   \n",
       "518932             NaN           NaN   \n",
       "...                ...           ...   \n",
       "1376256            NaN           NaN   \n",
       "1385567            NaN           NaN   \n",
       "1394370            NaN           NaN   \n",
       "1424093            NaN           NaN   \n",
       "1441792            NaN           NaN   \n",
       "\n",
       "                                                 permalink  sentiment  score   \n",
       "188548   https://old.reddit.com/r/polls/comments/orl7mj...    -0.3612    2.0  \\\n",
       "188549                                                 NaN        NaN    NaN   \n",
       "494626                                                 NaN        NaN    NaN   \n",
       "494627                                                 NaN        NaN    NaN   \n",
       "518932                                                 NaN        NaN    NaN   \n",
       "...                                                    ...        ...    ...   \n",
       "1376256                                                NaN        NaN    NaN   \n",
       "1385567                                                NaN        NaN    NaN   \n",
       "1394370                                                NaN        NaN    NaN   \n",
       "1424093                                                NaN        NaN    NaN   \n",
       "1441792                                                NaN        NaN    NaN   \n",
       "\n",
       "        created_date  created_day  created_month  created_year created_time   \n",
       "188548    2021-07-26         26.0            7.0        2021.0     11:17:44  \\\n",
       "188549           NaN          NaN            NaN           NaN          NaN   \n",
       "494626           NaN          NaN            NaN           NaN          NaN   \n",
       "494627           NaN          NaN            NaN           NaN          NaN   \n",
       "518932           NaN          NaN            NaN           NaN          NaN   \n",
       "...              ...          ...            ...           ...          ...   \n",
       "1376256          NaN          NaN            NaN           NaN          NaN   \n",
       "1385567          NaN          NaN            NaN           NaN          NaN   \n",
       "1394370          NaN          NaN            NaN           NaN          NaN   \n",
       "1424093          NaN          NaN            NaN           NaN          NaN   \n",
       "1441792          NaN          NaN            NaN           NaN          NaN   \n",
       "\n",
       "        body_clean  \n",
       "188548         NaN  \n",
       "188549         NaN  \n",
       "494626         NaN  \n",
       "494627         NaN  \n",
       "518932         NaN  \n",
       "...            ...  \n",
       "1376256        NaN  \n",
       "1385567        NaN  \n",
       "1394370        NaN  \n",
       "1424093        NaN  \n",
       "1441792        NaN  \n",
       "\n",
       "[329562 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean_comments.isna()\n",
    "clean_comments[clean_comments.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comments = clean_comments.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp because import creates float variables\n",
    "clean_comments[\"created_year\"] = pd.to_datetime(clean_comments[\"created_date\"]).dt.strftime('%Y')\n",
    "clean_comments[\"created_month\"] = pd.to_datetime(clean_comments[\"created_date\"]).dt.strftime('%m')\n",
    "clean_comments[\"created_day\"] = pd.to_datetime(clean_comments[\"created_date\"]).dt.strftime('%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit.name</th>\n",
       "      <th>subreddit.nsfw</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_day</th>\n",
       "      <th>created_month</th>\n",
       "      <th>created_year</th>\n",
       "      <th>created_time</th>\n",
       "      <th>body_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i79uz1c</td>\n",
       "      <td>oddlyterrifying</td>\n",
       "      <td>False</td>\n",
       "      <td>1.651658e+09</td>\n",
       "      <td>https://old.reddit.com/r/oddlyterrifying/comme...</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>04</td>\n",
       "      <td>05</td>\n",
       "      <td>2022</td>\n",
       "      <td>09:58:26</td>\n",
       "      <td>Oh shit there's a new one out? Last one k watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hz51unj</td>\n",
       "      <td>technews</td>\n",
       "      <td>False</td>\n",
       "      <td>1.646280e+09</td>\n",
       "      <td>https://old.reddit.com/r/technews/comments/t53...</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>03</td>\n",
       "      <td>03</td>\n",
       "      <td>2022</td>\n",
       "      <td>03:54:08</td>\n",
       "      <td>We’re never going to reopen those wells, its e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i3ic64d</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>False</td>\n",
       "      <td>1.649177e+09</td>\n",
       "      <td>https://old.reddit.com/r/worldnews/comments/tw...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>05</td>\n",
       "      <td>04</td>\n",
       "      <td>2022</td>\n",
       "      <td>16:36:35</td>\n",
       "      <td>Climate Change is the Great Filter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3tlo2</td>\n",
       "      <td>ontario</td>\n",
       "      <td>False</td>\n",
       "      <td>1.655760e+09</td>\n",
       "      <td>https://old.reddit.com/r/ontario/comments/vglj...</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>20</td>\n",
       "      <td>06</td>\n",
       "      <td>2022</td>\n",
       "      <td>21:16:31</td>\n",
       "      <td>Climate change also means greater crop yields ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iebulu4</td>\n",
       "      <td>news</td>\n",
       "      <td>False</td>\n",
       "      <td>1.656603e+09</td>\n",
       "      <td>https://old.reddit.com/r/news/comments/vo98pd/...</td>\n",
       "      <td>-0.6115</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>30</td>\n",
       "      <td>06</td>\n",
       "      <td>2022</td>\n",
       "      <td>15:26:18</td>\n",
       "      <td>The decline into total destruction by climate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   subreddit.name subreddit.nsfw   created_utc   \n",
       "0  i79uz1c  oddlyterrifying          False  1.651658e+09  \\\n",
       "1  hz51unj         technews          False  1.646280e+09   \n",
       "2  i3ic64d        worldnews          False  1.649177e+09   \n",
       "3  id3tlo2          ontario          False  1.655760e+09   \n",
       "4  iebulu4             news          False  1.656603e+09   \n",
       "\n",
       "                                           permalink  sentiment  score   \n",
       "0  https://old.reddit.com/r/oddlyterrifying/comme...    -0.5574    3.0  \\\n",
       "1  https://old.reddit.com/r/technews/comments/t53...     0.4588    1.0   \n",
       "2  https://old.reddit.com/r/worldnews/comments/tw...     0.6249    1.0   \n",
       "3  https://old.reddit.com/r/ontario/comments/vglj...     0.2960    0.0   \n",
       "4  https://old.reddit.com/r/news/comments/vo98pd/...    -0.6115   12.0   \n",
       "\n",
       "  created_date created_day created_month created_year created_time   \n",
       "0   2022-05-04          04            05         2022     09:58:26  \\\n",
       "1   2022-03-03          03            03         2022     03:54:08   \n",
       "2   2022-04-05          05            04         2022     16:36:35   \n",
       "3   2022-06-20          20            06         2022     21:16:31   \n",
       "4   2022-06-30          30            06         2022     15:26:18   \n",
       "\n",
       "                                          body_clean  \n",
       "0  Oh shit there's a new one out? Last one k watc...  \n",
       "1  We’re never going to reopen those wells, its e...  \n",
       "2                Climate Change is the Great Filter.  \n",
       "3  Climate change also means greater crop yields ...  \n",
       "4  The decline into total destruction by climate ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comments.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = clean_comments[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index value 871 is present in the DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "if 871 in all_docs.index:\n",
    "    print(\"Index value 871 is present in the DataFrame index.\")\n",
    "else:\n",
    "    print(\"Index value 871 is missing from the DataFrame index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=50000, step=1)\n"
     ]
    }
   ],
   "source": [
    "# Reset the index of the all_docs DataFrame\n",
    "all_docs = all_docs.reset_index(drop=True)\n",
    "\n",
    "# Verify the new index\n",
    "print(all_docs.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents\n",
    "#all_docs = subset[\"body_clean\"]\n",
    "doc_chunks = [all_docs[i:i+1000] for i in range(0, len(all_docs), 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "\n",
    "# Prepare sub-models that support online learning\n",
    "umap_model = IncrementalPCA(n_components=5)\n",
    "cluster_model = MiniBatchKMeans(n_clusters=50, random_state=0)\n",
    "vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "871",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/range.py:345\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range\u001b[39m.\u001b[39;49mindex(new_key)\n\u001b[1;32m    346\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 871 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Incrementally fit the topic model by training on 1000 documents at a time\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m docs \u001b[39min\u001b[39;00m doc_chunks:\n\u001b[0;32m----> 9\u001b[0m     topic_model\u001b[39m.\u001b[39;49mpartial_fit(docs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/bertopic/_bertopic.py:538\u001b[0m, in \u001b[0;36mBERTopic.partial_fit\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_representations_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m select_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model,\n\u001b[1;32m    537\u001b[0m                                               language\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage)\n\u001b[0;32m--> 538\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_embeddings(documents\u001b[39m.\u001b[39;49mDocument,\n\u001b[1;32m    539\u001b[0m                                           method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    540\u001b[0m                                           verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)\n\u001b[1;32m    541\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_representations_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/bertopic/_bertopic.py:2828\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[0;34m(self, documents, method, verbose)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model\u001b[39m.\u001b[39membed_words(documents, verbose)\n\u001b[1;32m   2827\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2828\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_model\u001b[39m.\u001b[39;49membed_documents(documents, verbose)\n\u001b[1;32m   2829\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2830\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong method for extracting document/word embeddings. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2831\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mEither choose \u001b[39m\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m'\u001b[39m\u001b[39m as the method. \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/bertopic/backend/_base.py:69\u001b[0m, in \u001b[0;36mBaseEmbedder.embed_documents\u001b[0;34m(self, document, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_documents\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     56\u001b[0m                     document: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     57\u001b[0m                     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(document, verbose)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/bertopic/backend/_sentencetransformers.py:63\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     50\u001b[0m           documents: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     51\u001b[0m           verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     52\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_model\u001b[39m.\u001b[39;49mencode(documents, show_progress_bar\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/range.py:347\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 347\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    349\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 871"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(umap_model=umap_model,\n",
    "                       hdbscan_model=cluster_model,\n",
    "                       vectorizer_model=vectorizer_model)\n",
    "\n",
    "# Incrementally fit the topic model by training on 1000 documents at a time\n",
    "for docs in doc_chunks:\n",
    "    topic_model.partial_fit(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Topic Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove climate change as word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subsets for every year\n",
    "year_groups = clean_comments.groupby(clean_comments['created_year'])\n",
    "\n",
    "year_dfs = {'comments_{}'.format(year): group for year, group in year_groups}\n",
    "\n",
    "for year, group in year_groups:\n",
    "    year_dfs[year] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_2010 = year_dfs[\"comments_2010\"][\"body_clean\"].values\n",
    "docs_2011 = year_dfs[\"comments_2011\"][\"body_clean\"].values\n",
    "docs_2012 = year_dfs[\"comments_2012\"][\"body_clean\"].values\n",
    "docs_2013 = year_dfs[\"comments_2013\"][\"body_clean\"].values\n",
    "docs_2014 = year_dfs[\"comments_2014\"][\"body_clean\"].values\n",
    "docs_2015 = year_dfs[\"comments_2015\"][\"body_clean\"].values\n",
    "docs_2016 = year_dfs[\"comments_2016\"][\"body_clean\"].values\n",
    "docs_2017 = year_dfs[\"comments_2017\"][\"body_clean\"].values\n",
    "docs_2018 = year_dfs[\"comments_2018\"][\"body_clean\"].values\n",
    "docs_2019 = year_dfs[\"comments_2019\"][\"body_clean\"].values\n",
    "docs_2020 = year_dfs[\"comments_2020\"][\"body_clean\"].values\n",
    "docs_2021 = year_dfs[\"comments_2021\"][\"body_clean\"].values\n",
    "docs_2022 = year_dfs[\"comments_2022\"][\"body_clean\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41705992cc5243dd994785f0f68ca73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 12:14:02,365 - BERTopic - Transformed documents to Embeddings\n",
      "2023-05-08 12:14:13,684 - BERTopic - Reduced dimensionality\n",
      "2023-05-08 12:14:14,106 - BERTopic - Clustered reduced embeddings\n",
      "2023-05-08 12:14:17,964 - BERTopic - Reduced number of topics from 103 to 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>8246</td>\n",
       "      <td>-1_climate_change_people_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5394</td>\n",
       "      <td>0_climate_change_global_warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>1_canada_harper_canadian_canadians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>2_religious_religion_god_people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>152</td>\n",
       "      <td>3_models_model_predictions_predict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>4_population_overpopulation_children_people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>140</td>\n",
       "      <td>5_data_emails_cru_scientists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>6_skeptics_skeptic_skeptical_skepticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>7_gore_al_change_climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>8_libertarians_libertarian_libertarianism_market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>96</td>\n",
       "      <td>9_china_taiwan_chinese_west</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>81</td>\n",
       "      <td>10_meat_livestock_animals_animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>11_copenhagen_summit_countries_accord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>65</td>\n",
       "      <td>12_denial_troll_change_climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>63</td>\n",
       "      <td>13_water_drought_rainfall_lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>59</td>\n",
       "      <td>14_fox_news_unbiased_broadcasting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>59</td>\n",
       "      <td>15_crops_gm_gmo_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>58</td>\n",
       "      <td>16_ocean_acidification_coral_iron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>17_koch_brothers_industries_nova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "      <td>18_terrorism_police_terrorists_iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>19_wear_clothes_dry_seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>20_holocaust_aids_hiv_denial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>21_pachauri_himalayan_glaciers_ipcc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>22_video_nile_film_change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>23_cancer_uk_health_html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>24_women_men_gender_male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>25_downvotes_downvoted_reddit_upvote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>26_volcanoes_volcano_volcanic_eruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>27_scam_change_climate_scammers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28_trees_forests_tree_carbon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>29_denier_change_climate_claims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>30_sensitivity_forcing_radiative_lu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>31_polar_bears_bear_species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>32_climatologists_geologists_climatologist_cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>33_wikileaks_cables_leaks_china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>34_nasa_space_exploration_budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>35_hoax_global_chan_change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>36_methane_clathrate_gwp_farts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>37_lights_hour_bulbs_light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>38_dinosaurs_lizards_extinction_impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>39_graph_charts_amp_ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>40_myth_myths_australia_perplexed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>41_anarchism_anarchists_anarchist_anarchy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>42</td>\n",
       "      <td>16</td>\n",
       "      <td>42_wager_pascal_argument_god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>43_aps_petition_council_committee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1   8246                   -1_climate_change_people_science\n",
       "1       0   5394                    0_climate_change_global_warming\n",
       "2       1    172                 1_canada_harper_canadian_canadians\n",
       "3       2    155                    2_religious_religion_god_people\n",
       "4       3    152                 3_models_model_predictions_predict\n",
       "5       4    152        4_population_overpopulation_children_people\n",
       "6       5    140                       5_data_emails_cru_scientists\n",
       "7       6     99            6_skeptics_skeptic_skeptical_skepticism\n",
       "8       7     99                           7_gore_al_change_climate\n",
       "9       8     98   8_libertarians_libertarian_libertarianism_market\n",
       "10      9     96                        9_china_taiwan_chinese_west\n",
       "11     10     81                   10_meat_livestock_animals_animal\n",
       "12     11     76              11_copenhagen_summit_countries_accord\n",
       "13     12     65                     12_denial_troll_change_climate\n",
       "14     13     63                     13_water_drought_rainfall_lake\n",
       "15     14     59                  14_fox_news_unbiased_broadcasting\n",
       "16     15     59                               15_crops_gm_gmo_food\n",
       "17     16     58                  16_ocean_acidification_coral_iron\n",
       "18     17     46                   17_koch_brothers_industries_nova\n",
       "19     18     45                18_terrorism_police_terrorists_iraq\n",
       "20     19     43                        19_wear_clothes_dry_seattle\n",
       "21     20     40                       20_holocaust_aids_hiv_denial\n",
       "22     21     35                21_pachauri_himalayan_glaciers_ipcc\n",
       "23     22     32                          22_video_nile_film_change\n",
       "24     23     32                           23_cancer_uk_health_html\n",
       "25     24     31                           24_women_men_gender_male\n",
       "26     25     30               25_downvotes_downvoted_reddit_upvote\n",
       "27     26     28             26_volcanoes_volcano_volcanic_eruption\n",
       "28     27     28                    27_scam_change_climate_scammers\n",
       "29     28     28                       28_trees_forests_tree_carbon\n",
       "30     29     25                    29_denier_change_climate_claims\n",
       "31     30     25                30_sensitivity_forcing_radiative_lu\n",
       "32     31     24                        31_polar_bears_bear_species\n",
       "33     32     23  32_climatologists_geologists_climatologist_cli...\n",
       "34     33     21                    33_wikileaks_cables_leaks_china\n",
       "35     34     21                   34_nasa_space_exploration_budget\n",
       "36     35     20                         35_hoax_global_chan_change\n",
       "37     36     20                     36_methane_clathrate_gwp_farts\n",
       "38     37     20                         37_lights_hour_bulbs_light\n",
       "39     38     20             38_dinosaurs_lizards_extinction_impact\n",
       "40     39     19                             39_graph_charts_amp_ja\n",
       "41     40     18                  40_myth_myths_australia_perplexed\n",
       "42     41     16          41_anarchism_anarchists_anarchist_anarchy\n",
       "43     42     16                       42_wager_pascal_argument_god\n",
       "44     43     16                  43_aps_petition_council_committee"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = (docs_2010,docs_2011,docs_2012,docs_2013,docs_2014,docs_2015,docs_2016,docs_2017,docs_2018,docs_2019,docs_2020,docs_2021,docs_2022)\n",
    "\n",
    "topics_2010,topics_2011,topics_2012, topics_2013, topics_2014, topics_2015, topics_2016, topics_2017, topics_2018, topics_2019, topics_2020, topics_2012, topics_2022 = None\n",
    "topics = (topics_2010,topics_2011,topics_2012, topics_2013, topics_2014, topics_2015, topics_2016, topics_2017, topics_2018, topics_2019, topics_2020, topics_2012, topics_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs, t in topics:\n",
    "\n",
    "  # BERT stepwise\n",
    "  # Step 1 - Extract embeddings\n",
    "  embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "  # Step 2 - Reduce dimensionality\n",
    "  umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "  # Step 3 - Cluster reduced embeddings\n",
    "  hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "  # Step 4 - Tokenize topics\n",
    "  vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "  # Step 5 - Create topic representation\n",
    "  ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "  # All steps together\n",
    "  t = BERTopic(\n",
    "    embedding_model=embedding_model,    # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,              # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,        # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,  # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,          # Step 5 - Extract topic words\n",
    "    calculate_probabilities=False,      # Raises speed\n",
    "    min_topic_size = 300,               # Reduces number of topics\n",
    "    nr_topics=\"auto\",                    # Reduces number of topics\n",
    "    verbose=True\n",
    "  )\n",
    "\n",
    "  topics, probs = t.fit_transform(d)\n",
    "  #t.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 0.01219470217097759),\n",
       " ('people', 0.010584660869951001),\n",
       " ('gt', 0.009517132731414605),\n",
       " ('don', 0.0086000351186167),\n",
       " ('scientific', 0.008597060961067942),\n",
       " ('global', 0.00822324026848448),\n",
       " ('think', 0.00787297838775973),\n",
       " ('scientists', 0.007625308000465114),\n",
       " ('evolution', 0.007521849591286694),\n",
       " ('warming', 0.007371242329106124)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get specific topic\n",
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store topic info in dataframe\n",
    "doc_info = topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Industrial output --&amp;gt; Increased atmospheric...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_people_gt_global_just</td>\n",
       "      <td>people - gt - global - just - science - don - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is true but only because Australia lacks ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_people_gt_global_just</td>\n",
       "      <td>people - gt - global - just - science - don - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please, explain to us the whole concept.\\n\\nNe...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_people_gt_global_just</td>\n",
       "      <td>people - gt - global - just - science - don - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It hasn't been \"d\" some political types prefer...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_weather_warming_global_snow</td>\n",
       "      <td>weather - warming - global - snow - ice - temp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&amp;gt; It's called \" \" or as the right wing has ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_weather_warming_global_snow</td>\n",
       "      <td>weather - warming - global - snow - ice - temp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Topic   \n",
       "0  Industrial output --&gt; Increased atmospheric...     -1  \\\n",
       "1  This is true but only because Australia lacks ...     -1   \n",
       "2  Please, explain to us the whole concept.\\n\\nNe...     -1   \n",
       "3  It hasn't been \"d\" some political types prefer...      1   \n",
       "4  &gt; It's called \" \" or as the right wing has ...      1   \n",
       "\n",
       "                            Name   \n",
       "0       -1_people_gt_global_just  \\\n",
       "1       -1_people_gt_global_just   \n",
       "2       -1_people_gt_global_just   \n",
       "3  1_weather_warming_global_snow   \n",
       "4  1_weather_warming_global_snow   \n",
       "\n",
       "                                         Top_n_words  Probability   \n",
       "0  people - gt - global - just - science - don - ...          0.0  \\\n",
       "1  people - gt - global - just - science - don - ...          0.0   \n",
       "2  people - gt - global - just - science - don - ...          0.0   \n",
       "3  weather - warming - global - snow - ice - temp...          1.0   \n",
       "4  weather - warming - global - snow - ice - temp...          1.0   \n",
       "\n",
       "   Representative_document  \n",
       "0                    False  \n",
       "1                    False  \n",
       "2                    False  \n",
       "3                    False  \n",
       "4                    False  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out document information\n",
    "doc_info.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
